{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ctt0lchCa_lm",
    "outputId": "f9555b74-1fe1-412c-d51d-6fcefb979249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting expertai-nlapi\n",
      "  Downloading expertai_nlapi-2.4.1-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from expertai-nlapi) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->expertai-nlapi) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->expertai-nlapi) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->expertai-nlapi) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->expertai-nlapi) (3.2)\n",
      "Installing collected packages: expertai-nlapi\n",
      "Successfully installed expertai-nlapi-2.4.1\n",
      "Collecting snscrape\n",
      "  Downloading snscrape-0.4.3.20220106-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from snscrape) (3.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from snscrape) (4.10.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\programdata\\anaconda3\\lib\\site-packages (from snscrape) (2.26.0)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from snscrape) (4.6.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Installing collected packages: snscrape\n",
      "Successfully installed snscrape-0.4.3.20220106\n"
     ]
    }
   ],
   "source": [
    "!pip install expertai-nlapi\n",
    "\n",
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sD1TW3lsatnd"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# !pip install expertai-nlapi\n",
    "\n",
    "# !pip install snscrape\n",
    "\n",
    "from flask import Flask, request\n",
    "import warnings\n",
    "import time\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import json\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from jinja2 import Template\n",
    "from datetime import datetime, date, timedelta\n",
    "import csv\n",
    "\n",
    "from expertai.nlapi.cloud.client import ExpertAiClient\n",
    "client = ExpertAiClient()\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"EAI_USERNAME\"] = 'dinesh.vijayakumar@live.com'\n",
    "os.environ[\"EAI_PASSWORD\"] = 'thXNs6X!7Mg3'\n",
    "\n",
    "base_path = \"/\"\n",
    "domain = \"builderprogram-dvijayakumar.quickbase.com\"\n",
    "usertoken = \"bz64sg_m2ax_0_b3v5uf6fjm5jq3nrwnkn7ixap\"\n",
    "headers = {'QB-Realm-Hostname': domain, 'User-Agent': 'PythonUtility',\n",
    "           'Authorization': 'QB-USER-TOKEN ' + usertoken}\n",
    "disastersTableid = \"bsq4ndg9r\"\n",
    "disastersReportid = \"5\"\n",
    "perBatchRecordsCount = 5000\n",
    "filename = \"Disaster Events.csv\"\n",
    "\n",
    "tweetsTableid=\"bsq4namtn\"\n",
    "entitiesTableid=\"bssdwittw\"\n",
    "behaviorsTableid=\"bssdx6irn\"\n",
    "emotionsTableid=\"bssdx7eu6\"\n",
    "\n",
    "tweetsClist=\"8.6.9.10.24.20.21.22.23.14.7.17\"\n",
    "entitiesClist=\"8.6.7\"\n",
    "behaviorsClist=\"9.6.7\"\n",
    "emotionsClist=\"8.6.7\"\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "\n",
    "    warnings.simplefilter('ignore')\n",
    "   \n",
    "    disasterdf = exportqbdata()\n",
    "\n",
    "    twitter_handles=[]\n",
    "    tweets_df=pd.DataFrame({})\n",
    "    entities_df=pd.DataFrame({})\n",
    "    behavioral_traits_df=pd.DataFrame({})\n",
    "    emotional_traits_df=pd.DataFrame({})\n",
    "\n",
    "    for index,row in disasterdf.iterrows():\n",
    "        twitter_hashtags=row[\"Combined Hashtag\"]\n",
    "        tweet_loc=row[\"Near Location\"]\n",
    "        twitter_duration_days=row[\"Monitor Duration Days (backwards from today)\"]\n",
    "        #event_tweets_df,event_entity_df,event_behavioral_traits_df, event_emotional_traits_df=scrapeTweets(twitter_hashtags,tweet_loc,twitter_duration_days)\n",
    "        event_tweets_df=scrapeTweets(twitter_hashtags,tweet_loc,twitter_duration_days)\n",
    "        event_tweets_df[\"Related Disaster\"]=row[\"Record ID#\"]\n",
    "        tweets_df=tweets_df.append(event_tweets_df)\n",
    "    \n",
    "    tweets_entities_list=[]\n",
    "    tweets_emotional_traits_list=[]\n",
    "    tweets_behavioral_traits_list=[]\n",
    "\n",
    "    tweet_entities_df=pd.DataFrame({})\n",
    "    tweet_emotional_traits_df=pd.DataFrame({})\n",
    "    tweet_behavioral_traits_df=pd.DataFrame({})\n",
    "    tweets_df[\"lang\"]=\"\"\n",
    "    tweets_df[[\"retweet_count\",\"reply_count\",\"like_count\",\"quote_count\"]]=0\n",
    "    for i,row in tweets_df.iterrows():\n",
    "        temp_tweet_meta_data_list=getTweetByID(row['Tweet Id'])\n",
    "        tweets_df.at[i,\"lang\"]=temp_tweet_meta_data_list[0]\n",
    "        tweets_df.at[i,\"retweet_count\"]=temp_tweet_meta_data_list[1]\n",
    "        tweets_df.at[i,\"reply_count\"]=temp_tweet_meta_data_list[2]\n",
    "        tweets_df.at[i,\"like_count\"]=temp_tweet_meta_data_list[3]\n",
    "        tweets_df.at[i,\"quote_count\"]=temp_tweet_meta_data_list[4]\n",
    "        tweets_df.at[i,\"Sentiment\"]=findSentiment(row[\"Text\"],temp_tweet_meta_data_list[0])\n",
    "     \n",
    "    for i,row in tweets_df.iterrows():\n",
    "        temp_entities=findEntities(row[\"Tweet Id\"],row[\"Text\"],row[\"lang\"])\n",
    "        if len(temp_entities)!=0:\n",
    "            tweets_entities_list.extend(temp_entities)\n",
    "\n",
    "        temp_emotional_traits=findEmotionalTraits(row[\"Tweet Id\"],row[\"Text\"],row[\"lang\"])\n",
    "        if len(temp_emotional_traits)!=0:\n",
    "            tweets_emotional_traits_list.extend(temp_emotional_traits)\n",
    "\n",
    "        temp_behavioral_traits=findBehavioralTraits(row[\"Tweet Id\"],row[\"Text\"],row[\"lang\"])\n",
    "        if len(temp_behavioral_traits)!=0:\n",
    "            tweets_behavioral_traits_list.extend(temp_behavioral_traits)\n",
    "\n",
    "\n",
    "    tweet_entities_df=tweet_entities_df.append(pd.DataFrame(tweets_entities_list,columns=[\"Related Tweet\",\"Entity\",\"Entity Type\"]))\n",
    "    tweet_behavioral_traits_df=tweet_behavioral_traits_df.append(pd.DataFrame(tweets_behavioral_traits_list,columns=[\"Related Tweet\",\"Behavior\",\"Frequency\"]))\n",
    "    tweet_emotional_traits_df=tweet_emotional_traits_df.append(pd.DataFrame(tweets_emotional_traits_list,columns=[\"Related Tweet\",\"Emotion\",\"Frequency\"]))\n",
    "    \n",
    "    tweet_entities_df=tweet_entities_df[tweet_entities_df.duplicated()==False]\n",
    "    tweet_behavioral_traits_df=tweet_behavioral_traits_df[tweet_behavioral_traits_df.duplicated()==False]\n",
    "    tweet_emotional_traits_df=tweet_emotional_traits_df[tweet_emotional_traits_df.duplicated()==False]\n",
    "    print(str(tweet_entities_df.shape[0])+\" entity records found...\")\n",
    "    print(str(tweet_behavioral_traits_df.shape[0])+\" emotional trait records found...\")\n",
    "    print(str(tweet_emotional_traits_df.shape[0])+\" behavioral trait records found...\")\n",
    "\n",
    "    print(tweets_df.head())\n",
    "    print(tweet_entities_df.head())\n",
    "    print(tweet_behavioral_traits_df.head())\n",
    "    print(tweet_emotional_traits_df.head())\n",
    "\n",
    "    tweets_df[\"Datetime_MS_QB\"]=tweets_df[\"Datetime\"].astype(\"str\").str.replace(\" \",\"T\")\n",
    "    tweets_df[\"Datetime_IST_QB\"]=pd.DatetimeIndex(tweets_df[\"Datetime\"]).tz_convert('Asia/Kolkata').astype(str).str.replace(\"\\+05:30\",\"\").str.replace(\" \",\"T\")\n",
    "    tweets_df.drop(columns=[\"Datetime\"],inplace=True)\n",
    "\n",
    "    importTweetsStatus,tweetsMetaData=pushTweetsToQB(tweets_df,domain,usertoken,tweetsTableid,tweetsClist)\n",
    "    importEntitiesStatus,entitiesMetaData=pushTweetsToQB(tweet_entities_df,domain,usertoken,entitiesTableid,entitiesClist)\n",
    "    importBehaviorsStatus,behaviorsMetaData=pushTweetsToQB(tweet_behavioral_traits_df,domain,usertoken,behaviorsTableid,behaviorsClist)\n",
    "    importEmotionsStatus,emotionsMetaData=pushTweetsToQB(tweet_emotional_traits_df,domain,usertoken,emotionsTableid,emotionsClist)\n",
    "\n",
    "    print('Here in last running from app')\n",
    "\n",
    "    Success_Response = '{\"Succes\": true, \"message\": \"Completed\"}'\n",
    "    response_dict = json.loads(Success_Response)\n",
    "    print(response_dict)\n",
    "    return response_dict\n",
    "\n",
    "\n",
    "def importDataToQB(importdata, tableid, headers):\n",
    "    url = 'https://api.quickbase.com/v1/records'\n",
    "    queryBody = {\n",
    "        \"to\": tableid,\n",
    "        \"data\": importdata,\n",
    "        \"fieldToReturn\": [\n",
    "            3\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    rQuery = requests.post(url,\n",
    "                           headers=headers,\n",
    "                           json=queryBody\n",
    "                           )\n",
    "    importStatus = \"\"\n",
    "    metaData = \"\"\n",
    "    if rQuery.status_code == 200:\n",
    "        recordsProcessed = json.loads(\n",
    "            rQuery.text)['metadata']['totalNumberOfRecordsProcessed']\n",
    "        print(\"Number of records processed...\"+str(recordsProcessed))\n",
    "        importStatus = \"Success\"\n",
    "        metaData = recordsProcessed\n",
    "    else:\n",
    "        print(rQuery.text)\n",
    "        importStatus = \"Failed\"\n",
    "        metaData = rQuery.text\n",
    "    return importStatus, metaData\n",
    "\n",
    "\n",
    "def pushTweetsToQB(df,domain,usertoken,tableid,clist):\n",
    "\n",
    "    importdatadf=df.copy(deep=True)\n",
    "    headers = {'QB-Realm-Hostname': domain,'User-Agent': 'PythonUtility','Authorization': 'QB-USER-TOKEN '+usertoken}\n",
    "    perBatchRecordsCount=10000\n",
    "\n",
    "\n",
    "    rowsCount=importdatadf.shape[0]\n",
    "    print(\"Records to be imported...\"+str(rowsCount))\n",
    "    print(importdatadf.head())\n",
    "\n",
    "    importdatadf=importdatadf.fillna(value=\"\")\n",
    "    if rowsCount>perBatchRecordsCount:\n",
    "        for skip in range(0,rowsCount,perBatchRecordsCount):\n",
    "            importsubdata=importdatadf.loc[skip:skip+perBatchRecordsCount]\n",
    "            importdatadf[skip:].to_csv(\"temp1.csv\")\n",
    "            importsubdata.columns=list(clist.split(sep=\".\"))\n",
    "            importdataJSON=json.loads(importsubdata.to_json(orient='records'))\n",
    "    else:\n",
    "        importsubdata=importdatadf\n",
    "        importsubdata.columns=list(clist.split(sep=\".\"))\n",
    "        importdataJSON=json.loads(importsubdata.to_json(orient='records'))\n",
    "    for index,item in enumerate(importdataJSON):\n",
    "        for key in item.keys():\n",
    "            importdataJSON[index][key]=dict({\"value\":importdataJSON[index][key]})\n",
    "    importStatus, metaData=importDataToQB(importdataJSON,tableid,headers)\n",
    "    return importStatus, metaData\n",
    "\n",
    "def getTweetByID(id):\n",
    "    search_url = \"https://api.twitter.com/2/tweets\"\n",
    "    bearer_token=\"AAAAAAAAAAAAAAAAAAAAAOvvRQEAAAAAIYR6XMy63KCVbWaN2VdBqv76FgI%3DYiwwZdh0h1OC1W6ka5lGOGeShWecq878kytDtdzUBWoageLNWU\"\n",
    "    query_params =  {'ids':id,     \n",
    "                  'tweet.fields':'created_at,lang,text,geo,author_id,id,public_metrics,referenced_tweets',\n",
    "                  'expansions':'geo.place_id,author_id', \n",
    "                  'place.fields':'contained_within,country,country_code,full_name,geo,id,name,place_type',\n",
    "                  'user.fields':'description,username,id'}\n",
    "    #                'max_results':'500'}\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=query_params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_response= response.json()\n",
    "    df = pd.json_normalize(json_response['data'])\n",
    "    return [df[\"lang\"][0],df[\"public_metrics.retweet_count\"][0],df[\"public_metrics.reply_count\"][0],df[\"public_metrics.like_count\"][0],df[\"public_metrics.quote_count\"][0]]\n",
    "\n",
    "def findSentiment(x,lang):\n",
    "    text=x\n",
    "    language= lang\n",
    "    try:\n",
    "        output = client.specific_resource_analysis(\n",
    "          body={\"document\": {\"text\": text}}, \n",
    "          params={'language': language, 'resource': 'sentiment'})\n",
    "        return output.sentiment.overall\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def findEntities(id,x,lang):\n",
    "    text =x\n",
    "    language= lang\n",
    "    entities=[]\n",
    "    try:\n",
    "        output = client.specific_resource_analysis(\n",
    "          body={\"document\": {\"text\": text}}, \n",
    "          params={'language': language, 'resource': 'entities'})\n",
    "        for entity in output.entities:\n",
    "          entities.append([id,entity.lemma,entity.type_])\n",
    "        return entities\n",
    "    except:\n",
    "        return entities\n",
    "    \n",
    "def findEmotionalTraits(id,x,lang):\n",
    "    taxonomy='emotional-traits'\n",
    "    text =x\n",
    "    language= lang\n",
    "    emotions=[]\n",
    "    try:\n",
    "        document = client.classification(body={\"document\": {\"text\": text}}, params={'taxonomy': taxonomy,'language': language})\n",
    "        for category in document.categories:\n",
    "            emotions.append([id,category.label,category.frequency])\n",
    "        return emotions\n",
    "    except:\n",
    "        return emotions\n",
    "\n",
    "def findBehavioralTraits(id,x,lang):\n",
    "    taxonomy='behavioral-traits'\n",
    "    text =x\n",
    "    language= lang\n",
    "    behaviors=[]\n",
    "    try:\n",
    "        document = client.classification(body={\"document\": {\"text\": text}}, params={'taxonomy': taxonomy,'language': language})\n",
    "        for category in document.categories:\n",
    "            behaviors.append([id,category.label,category.frequency])\n",
    "        return behaviors\n",
    "    except:\n",
    "        return behaviors\n",
    "\n",
    "def getQBBatchDF(batchRecordCount, skipStart, firstIter):\n",
    "    print(\"Generating the records from \" + str(int(skipStart) + 1) + \"...\")\n",
    "    url = 'https://api.quickbase.com/v1/reports/' + disastersReportid + '/run?tableId=' + disastersTableid + '&skip=' + str(\n",
    "        skipStart) + '&top=' + str(batchRecordCount)\n",
    "    queryBody = {\n",
    "    }\n",
    "\n",
    "    retryCount = 0\n",
    "    qbdataDF = pd.DataFrame()\n",
    "    qbrecordsDict = dict()\n",
    "    qbfieldsDict = []\n",
    "\n",
    "    try:\n",
    "        print(url)\n",
    "        rQuery = requests.post(url,\n",
    "                               headers=headers,\n",
    "                               json=queryBody,\n",
    "                               verify=False\n",
    "                               )\n",
    "\n",
    "        if rQuery.status_code == 200:\n",
    "            responseQuery = json.loads(json.dumps(rQuery.json(), indent=4))\n",
    "            qbfieldsDict = list(responseQuery[\"fields\"])\n",
    "            qbrecordsDict = (responseQuery[\"data\"])\n",
    "            qbdataDF = pd.DataFrame.from_dict(qbrecordsDict, orient=\"columns\")\n",
    "            for col in qbdataDF.columns:\n",
    "                qbdataDF[col] = pd.json_normalize(qbdataDF[col], max_level=0)\n",
    "        else:\n",
    "            print(\"Skipped the records from \" + str(int(skipStart) + 1) + \" to \" + str(\n",
    "                int(skipStart) + perBatchRecordsCount) + \"...\")\n",
    "\n",
    "        return qbdataDF, qbfieldsDict, rQuery.status_code\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        while retryCount < 3:\n",
    "            retryCount += 1\n",
    "            print(\"Retry \"+str(retryCount))\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "\n",
    "def exportqbdata(skipStart=0, batchRecordCount=0):\n",
    "    if batchRecordCount == 0:\n",
    "        batchRecordCount = perBatchRecordsCount\n",
    "    if (batchRecordCount <= perBatchRecordsCount):\n",
    "        outputdf, qbfieldsDict, statusCode = getQBBatchDF(\n",
    "            batchRecordCount, skipStart, firstIter=True)\n",
    "        outputdf.to_csv(filename, encoding=\"utf-8-sig\")\n",
    "    else:\n",
    "        outputdf = pd.DataFrame()\n",
    "        lastbatch = False\n",
    "        skipStartIter = int(skipStart)\n",
    "        firstIter = True\n",
    "        print(batchRecordCount, skipStart)\n",
    "        while lastbatch == False:\n",
    "            qbbatchdf, qbfieldsDict, statusCode = getQBBatchDF(\n",
    "                perBatchRecordsCount, skipStartIter, firstIter)\n",
    "            print(statusCode)\n",
    "            if statusCode == 200:\n",
    "                firstIter = False\n",
    "                outputdf = outputdf.append(qbbatchdf)\n",
    "                outputdf.to_csv(\"Disaster Events.csv\", encoding=\"utf-8-sig\")\n",
    "                skipStartIter += perBatchRecordsCount\n",
    "                if skipStartIter > (int(batchRecordCount)+int(skipStart)):\n",
    "                    lastbatch = True\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    qbfieldsDF = pd.DataFrame.from_dict(qbfieldsDict)\n",
    "    qbfieldsDF.set_index(\"id\", inplace=True)\n",
    "    qbcols = outputdf.columns\n",
    "    qbcolnames = []\n",
    "    for col in qbcols:\n",
    "        qbcolnames.append(qbfieldsDF[\"label\"][int(col)])\n",
    "    outputdf.columns = qbcolnames\n",
    "    outputdf.reset_index(drop=True, inplace=True)\n",
    "    print(outputdf.shape)\n",
    "    outputdf.to_csv(filename, encoding=\"utf-8-sig\")\n",
    "    print(\"Exporting \"+str(batchRecordCount)+\" records to a CSV file...\")\n",
    "    return outputdf\n",
    "\n",
    "\n",
    "# Initiate webscrapping of tweets\n",
    "def scrapeTweets(twitter_hashtags,tweet_loc,twitter_duration_days): # Initiate webscrapping of tweets\n",
    "    tweets_list = []\n",
    "    tweets_df=pd.DataFrame({})\n",
    "\n",
    "    for i in range(len(twitter_hashtags)):\n",
    "        \n",
    "        print(twitter_duration_days)\n",
    "\n",
    "        batches=[]\n",
    "        batchesTimestamp=[]\n",
    "\n",
    "        end_date=datetime.today()+timedelta(days=1)\n",
    "        end_date_temp=end_date\n",
    "        duration_days_temp=twitter_duration_days+1\n",
    "\n",
    "        while duration_days_temp>=0:\n",
    "            end_date_temp=end_date-timedelta(days=duration_days_temp)\n",
    "            batches.append(datetime.strftime(end_date_temp,\"%Y-%m-%d\"))\n",
    "            batchesTimestamp.append(end_date_temp)\n",
    "            duration_days_temp=duration_days_temp-999\n",
    "\n",
    "        print(batches)\n",
    "\n",
    "        end_date_temp=datetime.strftime(end_date,\"%Y-%m-%d\")\n",
    "\n",
    "        for k in reversed(range(0,len(batches))):\n",
    "            print(\"Retrieving tweets between \"+batches[k]+\" and \"+end_date_temp+\" (\"+str((datetime.strptime(end_date_temp,\"%Y-%m-%d\")-datetime.strptime(batches[k],\"%Y-%m-%d\")).days)+\" days)\")\n",
    "            for tweet in sntwitter.TwitterSearchScraper(twitter_hashtags[i]+' near:\"'+tweet_loc+'\" within:50km since:'+batches[k]+' until:'+end_date_temp).get_items():\n",
    "                tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username])\n",
    "            \n",
    "            temp_df=pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "            tweets_df=tweets_df.append(temp_df)\n",
    "            end_date_temp=batches[k]\n",
    "        \n",
    "    tweets_df=tweets_df[tweets_df.duplicated()==False]\n",
    "\n",
    "    print(str(tweets_df.shape[0])+\" tweets fetched...\")\n",
    "\n",
    "    return tweets_df\n",
    "\n",
    "\n",
    "def importDataToQB(importdata, tableid, headers):\n",
    "    url = 'https://api.quickbase.com/v1/records'\n",
    "    queryBody = {\n",
    "        \"to\": tableid,\n",
    "        \"data\": importdata,\n",
    "        \"fieldToReturn\": [\n",
    "            3\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    rQuery = requests.post(url,\n",
    "                           headers=headers,\n",
    "                           json=queryBody\n",
    "                           )\n",
    "    importStatus = \"\"\n",
    "    metaData = \"\"\n",
    "    if rQuery.status_code == 200:\n",
    "        recordsProcessed = json.loads(\n",
    "            rQuery.text)['metadata']['totalNumberOfRecordsProcessed']\n",
    "        print(\"Number of records processed...\"+str(recordsProcessed))\n",
    "        importStatus = \"Success\"\n",
    "        metaData = recordsProcessed\n",
    "    else:\n",
    "        print(rQuery.text)\n",
    "        importStatus = \"Failed\"\n",
    "        metaData = rQuery.text\n",
    "    return importStatus, metaData\n",
    "\n",
    "\n",
    "def pushTweetsToQB(df,domain,usertoken,tableid,clist):\n",
    "\n",
    "    importdatadf=df.copy(deep=True)\n",
    "    headers = {'QB-Realm-Hostname': domain,'User-Agent': 'PythonUtility','Authorization': 'QB-USER-TOKEN '+usertoken}\n",
    "    perBatchRecordsCount=10000\n",
    "\n",
    "\n",
    "    rowsCount=importdatadf.shape[0]\n",
    "    print(\"Records to be imported...\"+str(rowsCount))\n",
    "    print(importdatadf.head())\n",
    "\n",
    "    importdatadf=importdatadf.fillna(value=\"\")\n",
    "    if rowsCount>perBatchRecordsCount:\n",
    "        for skip in range(0,rowsCount,perBatchRecordsCount):\n",
    "            importsubdata=importdatadf.loc[skip:skip+perBatchRecordsCount]\n",
    "            importdatadf[skip:].to_csv(\"temp1.csv\")\n",
    "            importsubdata.columns=list(clist.split(sep=\".\"))\n",
    "            importdataJSON=json.loads(importsubdata.to_json(orient='records'))\n",
    "    else:\n",
    "        importsubdata=importdatadf\n",
    "        importsubdata.columns=list(clist.split(sep=\".\"))\n",
    "        importdataJSON=json.loads(importsubdata.to_json(orient='records'))\n",
    "    for index,item in enumerate(importdataJSON):\n",
    "        for key in item.keys():\n",
    "            importdataJSON[index][key]=dict({\"value\":importdataJSON[index][key]})\n",
    "    importStatus, metaData=importDataToQB(importdataJSON,tableid,headers)\n",
    "    return importStatus, metaData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNnjtXLQdKNI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
